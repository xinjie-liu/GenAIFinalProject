{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jax Tutorial Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 10:47:12.049244: W external/xla/xla/service/gpu/nvptx_compiler.cc:765] The NVIDIA driver's CUDA version is 12.6 which is older than the ptxas CUDA version (12.8.93). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.zeros(shape=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 10:47:12.869372: W external/xla/xla/service/hlo_rematerialization.cc:3005] Can't reduce memory use below 2.94GiB (3160611635 bytes) by rematerialization; only reduced to 7.50GiB (8050400000 bytes), down from 7.50GiB (8050400000 bytes) originally\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.5338445e-05 4.1128933e-05 4.3717988e-05 ... 4.6037432e-05\n",
      "  3.9925937e-05 3.9466231e-05]\n",
      " [4.0832507e-05 5.2905863e-05 3.8572820e-05 ... 4.3123215e-05\n",
      "  6.3363848e-05 4.0338127e-05]\n",
      " [4.5396719e-05 5.3326974e-05 3.9350507e-05 ... 5.8048146e-05\n",
      "  4.2621217e-05 4.6469559e-05]\n",
      " ...\n",
      " [4.3514232e-05 5.5165212e-05 5.5767730e-05 ... 4.7878766e-05\n",
      "  4.5894838e-05 5.1567564e-05]\n",
      " [5.8635400e-05 3.8818544e-05 5.1258208e-05 ... 5.6500274e-05\n",
      "  5.4009601e-05 4.4502205e-05]\n",
      " [4.1771575e-05 4.4697368e-05 4.6291960e-05 ... 4.1309504e-05\n",
      "  5.5091899e-05 4.0407260e-05]]\n",
      "[1.         1.0000001  0.99999994 ... 1.         0.99999994 1.        ]\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def softmax(array, temp=2.0):\n",
    "    arr = jnp.exp(array / temp)\n",
    "    return arr/ jnp.sum(arr, axis=-1, keepdims=True)\n",
    "# Cant jhust jit enveryhting, no while loops and conditionals for now\n",
    "\n",
    "key = jax.random.key(50)\n",
    "k, sk = jax.random.split(key)\n",
    "print(softmax(jax.random.uniform(sk, shape=(100000,20000))))\n",
    "print(softmax(jax.random.uniform(sk, shape=(100000,20000))).sum(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3, 3)\n",
      "(10000, 12, 12)\n",
      "Time taken to compute: 0.8872263431549072 seconds\n",
      "(10000, 3, 3)\n",
      "(10000, 12, 12)\n",
      "Time taken to compute: 0.7750630378723145 seconds\n"
     ]
    }
   ],
   "source": [
    "# Experiments with vmap\n",
    "# Convolution \n",
    "\n",
    "# Input 3D array (batch, dim1, dim2), kernel (batch, k1, k2)\n",
    "\n",
    "@jax.jit\n",
    "def convolve(array, kernel, answer):\n",
    "    b,k1,k2 = kernel.shape\n",
    "    for j in range(answer.shape[1]):\n",
    "        for k in range(answer.shape[2]):\n",
    "            cov = jnp.multiply(array[:, j:j+k1, k:k+k2], kernel).sum(axis=-1).sum(axis=-1)\n",
    "            answer = answer.at[:, j, k].set(cov)\n",
    "    \n",
    "    return answer   \n",
    "\n",
    "def convolve2D(array, kernel, padding =  False):\n",
    "    if kernel.ndim == 2:\n",
    "        kernel = jnp.tile(kernel, (array.shape[0], 1, 1)) \n",
    "        print(kernel.shape)\n",
    "    \n",
    "    if padding:\n",
    "        answer = jnp.zeros(shape = (array.shape))\n",
    "        pad_width = [(0,0)] + [((k-1)//2, (k-1)//2 + (1 if k%2==0 else 0)) for k in kernel.shape[1:]]\n",
    "        array = jnp.pad(array, pad_width=pad_width, mode='constant', constant_values=0)\n",
    "        print(array.shape)\n",
    "        \n",
    "\n",
    "    else:\n",
    "        shape = [array.shape[0], array.shape[1] - 2*((kernel.shape[1]-1)//2), array.shape[2] - 2*((kernel.shape[1]-1)//2)]\n",
    "        answer = jnp.zeros(shape= shape)\n",
    "        print(answer.shape)\n",
    "    \n",
    "    return convolve(array, kernel, answer)\n",
    "\n",
    "@jax.jit\n",
    "def convolve2(array, kernel, answer):\n",
    "    k1,k2 = kernel.shape\n",
    "    for j in range(answer.shape[0]):\n",
    "        for k in range(answer.shape[1]):\n",
    "            cov = jnp.multiply(array[j:j+k1, k:k+k2], kernel).sum(axis=-1).sum(axis=-1)\n",
    "            answer = answer.at[j, k].set(cov)\n",
    "    \n",
    "    return answer \n",
    "\n",
    "def convolve2D2(array, kernel, padding=False):\n",
    "\n",
    "    if kernel.ndim == 2:\n",
    "        kernel = jnp.tile(kernel, (array.shape[0], 1, 1)) \n",
    "        print(kernel.shape)\n",
    "        \n",
    "    if padding:\n",
    "        answer = jnp.zeros(shape = (array.shape))\n",
    "        pad_width = [(0,0)] + [((k-1)//2, (k-1)//2 + (1 if k%2==0 else 0)) for k in kernel.shape[1:]]\n",
    "        array = jnp.pad(array, pad_width=pad_width, mode='constant', constant_values=0)\n",
    "        print(array.shape)\n",
    "        \n",
    "\n",
    "    else:\n",
    "        shape = [array.shape[0], array.shape[1] - 2*((kernel.shape[1]-1)//2), array.shape[2] - 2*((kernel.shape[1]-1)//2)]\n",
    "        answer = jnp.zeros(shape= shape)\n",
    "        print(answer.shape)\n",
    "    \n",
    "    convolve2batch = jax.vmap(convolve2)\n",
    "    return convolve2batch(array, kernel, answer)\n",
    "\n",
    "array = jnp.tile(jnp.arange(10), (10000, 10, 1))\n",
    "\n",
    "kernel = jnp.array([[1,1,1],[1,1,1], [1,1,1]])\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "result = convolve2D(array, kernel, padding= True)\n",
    "print(f\"Time taken to compute: {time.time() - start_time} seconds\")\n",
    "\n",
    "\n",
    "# convolve2Dbatch = jax.vmap(convolve2D2)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "result = convolve2D2(array, kernel, padding= True)\n",
    "print(f\"Time taken to compute: {time.time() - start_time} seconds\")\n",
    "\n",
    "# Speedup is insane, mrola of the story do element wise computation, for each batch basically just do a vmap\n",
    "# Recurrent neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments with vmap\n",
    "# Recurrent Neural Network\n",
    "# Input 3D array (batch, dim1, dim2), kernel (batch, k1, k2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
